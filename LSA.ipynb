{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    "\n",
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')  \n",
    "article = scraped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:  \n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text) \n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texts = []\n",
    "for i in sentence_list:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel(num_terms=2152, num_topics=7, decay=1.0, chunksize=20000)\n",
      "[(0, '-0.524*\"ai\" + -0.310*\"human\" + -0.299*\"intellig\" + -0.238*\"use\" + -0.213*\"machin\" + -0.192*\"research\" + -0.177*\"artifici\" + -0.176*\"learn\" + -0.162*\"comput\" + -0.135*\"problem\" + -0.122*\"mani\" + -0.119*\"system\" + -0.105*\"gener\" + -0.101*\"network\" + -0.089*\"algorithm\" + -0.068*\"goal\" + -0.068*\"knowledg\" + -0.068*\"one\" + -0.067*\"ethic\" + -0.067*\"neural\"'), (1, '0.589*\"ai\" + -0.419*\"intellig\" + -0.364*\"human\" + -0.344*\"machin\" + -0.238*\"artifici\" + -0.147*\"ethic\" + -0.096*\"comput\" + 0.091*\"research\" + 0.076*\"use\" + 0.063*\"algorithm\" + -0.059*\"moral\" + 0.051*\"data\" + 0.050*\"goal\" + 0.050*\"theori\" + -0.050*\"gener\" + -0.049*\"natur\" + 0.048*\"knowledg\" + -0.048*\"could\" + -0.045*\"concern\" + -0.043*\"mind\"'), (2, '0.443*\"learn\" + 0.407*\"use\" + 0.383*\"network\" + -0.361*\"ai\" + 0.241*\"neural\" + 0.214*\"algorithm\" + -0.205*\"human\" + -0.122*\"intellig\" + 0.118*\"deep\" + 0.101*\"bayesian\" + 0.098*\"layer\" + -0.095*\"research\" + 0.081*\"problem\" + 0.071*\"logic\" + 0.064*\"data\" + 0.060*\"train\" + 0.056*\"exampl\" + -0.056*\"comput\" + 0.054*\"decis\" + 0.052*\"plan\"')]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "lsamodel = LsiModel(doc_term_matrix, num_topics= 7, id2word = dictionary)\n",
    "print(lsamodel)\n",
    "for i in range(0,4):
    "    a = lsamodel.print_topics(num_topics=4, num_words= 20)[i][1]",
    "    article_text = re.sub(r'\[[0-9]*\]', ' ', a)",  
    "    article_text = re.sub(r'\s+', ' ', article_text)",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )",  
    "    formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)",
    "    print(formatted_article_text, end=" ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
